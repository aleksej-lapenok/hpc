{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЦЕЛЬ ЛАБОРАТОРНОЙ РАБОТЫ\n",
    "Целью этой задачи было показать работу MPI_SEND, MPI_Send, MPI_Send,\n",
    "MPI_Rsend) и завершить анализ производительности каждого из них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПОСТАНОВКА ЗАДАЧИ\n",
    "Мы разрабатываем простое приложение, которое отправляет несколько байт данных из одного процесса в другой\n",
    "один."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# КРАТКАЯ ТЕОРИЯ\n",
    "\n",
    "Проект Open MPI - это реализация интерфейса передачи сообщений с открытым исходным кодом, которая\n",
    "разработан и поддерживается консорциумом научных, исследовательских и промышленных партнеров. Таким образом, Open MPI может объединить опыт, технологии и ресурсы всего высокопроизводительного вычислительного сообщества, чтобы создать лучшую доступную библиотеку MPI. Open MPI предлагает преимущества для поставщиков систем и программного обеспечения, разработчиков приложений и исследователей компьютерных наук.\n",
    "\n",
    "MPI имеет несколько различных \"режимов отправки.\" Они представляют собой различные варианты буферизации (где хранятся данные до их получения) и синхронизации (когда отправка завершена). В следующем примере я использую \"отправить буфер\" для пользовательского буфера для отправки. \n",
    "\n",
    "- MPI_Send\n",
    "  MPI_Send не завершится, пока вы не сможете использовать буфер отправки. Он может или не может блокировать (разрешено буферизовать либо на стороне отправителя или получателя, либо ждать соответствующего приема).\n",
    "- MPI_Bsend \n",
    "  May buffer; завершается сразу, и вы можете использовать буфер отправки. Появилось позднее дополнение к спецификации MPI. Следует использовать только в случае крайней необходимости. \n",
    "- MPI_Ssend\n",
    "  не завершится, пока сопоставление не будет отправлено\n",
    "- MPI_Rsend\n",
    "  Может использоваться, только если совпадение уже учтено. Пользователь отвечает за написание правильной программы.\n",
    "- MPI_Isend\n",
    "   Неблокирующая отправка. Но не обязательно асинхронный. Вы не можете повторно использовать буфер отправки до тех пор, пока либо успешно, вы ждете, что сообщение было получено (см. MPI_Request_free). Заметим также, что, хотя I относится к immediate, на MPI_Isend нет требований к производительности. Немедленная отправка должна быть возвращена пользователю без необходимости соответствующего получения в месте назначения.\n",
    "   \n",
    " Реализация может свободно отправлять данные в пункт назначения перед return, если вызов send не блокирует ожидание соответствующего приема. Отличающийся\n",
    "стратегии отправки данных предлагают различные преимущества и недостатки производительности, которые будут зависеть от приложения\n",
    "- Mpi_ibsend буферизованный неблокирующий\n",
    "- MPI_Issend синхронный неблокирующий. Обратите внимание, что ожидание завершится только после успешного получения.\n",
    "- MPI_Irsend как с MPI_Rsend, но неблокирующий.\n",
    "\n",
    "Обратите внимание, что \"неблокирующий\" относится только к тому, доступен ли буфер данных для повторного использования после вызова. Например, никакая часть спецификации MPI не предусматривает одновременной работы передачи данных и вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритм реализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <mpi.h>\r\n",
      "#include <stdio.h>\r\n",
      "#include <stdlib.h>\r\n",
      "\r\n",
      "#include <unistd.h>\r\n",
      "#include <stdbool.h>\r\n",
      "\r\n",
      "#ifndef MSG_LEN\r\n",
      "# define MSG_LEN 32\r\n",
      "#endif\r\n",
      "\r\n",
      "\r\n",
      "#ifndef SEND_FN\r\n",
      "# define SEND_FN MPI_Send\r\n",
      "#endif\r\n",
      "\r\n",
      "#if !defined(SYNC) && !defined(SEND_RECV) && !defined(ASYNC)\r\n",
      "# define SYNC\r\n",
      "#endif\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "void rand_str(char *str, size_t len) \r\n",
      "{\r\n",
      "\tfor(size_t i = 0; i < len - 1; ++i) {\r\n",
      "\t\tstr[i] = rand() % 26 + 64;\r\n",
      "\t}\r\n",
      "\tstr[len] = 0;\r\n",
      "}\r\n",
      "\r\n",
      "int main(int argc,char **argv)\r\n",
      "{\r\n",
      "\tint rank, size;\r\n",
      "\tMPI_Init(&argc,&argv);\r\n",
      "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\r\n",
      "\tMPI_Comm_size(MPI_COMM_WORLD,&size);\r\n",
      "\r\n",
      "\tMPI_Request req;\r\n",
      "\tMPI_Status status;\r\n",
      "\tbool wait = false;\r\n",
      "\tsrand(rank+10);\r\n",
      "\r\n",
      "\tchar buf[MSG_LEN], rbuf[MSG_LEN];\r\n",
      "\r\n",
      "#ifdef SYNC\r\n",
      "\tprintf(\"SYNC\\n\");\r\n",
      "#endif\r\n",
      "\r\n",
      "#ifdef SEND_RECV\r\n",
      "\tprintf(\"SEND_RECV\\n\");\r\n",
      "#endif\r\n",
      "\r\n",
      "#ifdef ASYNC\r\n",
      "\tprintf(\"ASYNC\\n\");\r\n",
      "#endif\r\n",
      "\r\n",
      "\tfor(size_t i = 0; i < 10; ++i) {\r\n",
      "\t\t\t\r\n",
      "#ifdef SYNC\r\n",
      "\t\tif( (i + rank) % 2 == 0 ) {\r\n",
      "\t\t\tMPI_Recv(buf, MSG_LEN, MPI_CHAR, !rank, 0, MPI_COMM_WORLD,MPI_STATUS_IGNORE);\r\n",
      "\t\t\tprintf(\"%*sRECV(%d) : %s\\n\",rank*44, \" \", rank, buf);\r\n",
      "\t\t} else {\r\n",
      "\t\t\trand_str(buf, MSG_LEN);\r\n",
      "\t\t\tprintf(\"%*sSEND(%d) : %s\\n\", rank*44, \" \", rank, buf);\r\n",
      "\t\t\tSEND_FN(buf, MSG_LEN, MPI_CHAR, !rank, 0, MPI_COMM_WORLD);\r\n",
      "\t\t}\r\n",
      "#endif\r\n",
      "\r\n",
      "#ifdef SEND_RECV\r\n",
      "\t\trand_str(buf, MSG_LEN);\r\n",
      "\t\tprintf(\"%*sSEND(%d) : %s\\n\", rank*44, \" \", rank, buf);\r\n",
      "\t\tMPI_Sendrecv(buf, MSG_LEN, MPI_CHAR, !rank, 0, \r\n",
      "\t\t\t\t\t\trbuf, MSG_LEN, MPI_CHAR, !rank, 0, \r\n",
      "\t\t\t\t\t\t\tMPI_COMM_WORLD, MPI_STATUS_IGNORE);\r\n",
      "\t\tprintf(\"%*sRECV(%d) : %s\\n\",rank*44, \" \", rank, rbuf);\r\n",
      "#endif\t\r\n",
      "\r\n",
      "\r\n",
      "#ifdef ASYNC\t\r\n",
      "\r\n",
      "\t\tif( (i + rank) % 2 == 0 ) {\r\n",
      "\t\t\tif(wait) {\r\n",
      "\t\t\t\tMPI_Wait(&req, &status);\r\n",
      "\t\t\t\tMPI_Irecv(buf, MSG_LEN, MPI_CHAR, !rank, 0, MPI_COMM_WORLD, &req);\r\n",
      "\t\t\t\twait = true;\r\n",
      "\t\t\t\tprintf(\"%*sRECV(%d) : %s\\n\",rank*44, \" \", rank, buf);\r\n",
      "\t\t\t}\r\n",
      "\t\t} else {\r\n",
      "\t\t\trand_str(buf, MSG_LEN);\r\n",
      "\t\t\tprintf(\"%*sSEND(%d) : %s\\n\", rank*44, \" \", rank, buf);\r\n",
      "\t\t\tif(wait) MPI_Wait(&req, &status);\r\n",
      "\t\t\tMPI_Isend(buf, MSG_LEN, MPI_CHAR, !rank, 0, MPI_COMM_WORLD, &req);\r\n",
      "\t\t\twait = true;\r\n",
      "\t\t}\r\n",
      "#endif\t\r\n",
      "\t\t// sleep(rand() % 5);\r\n",
      "\t}\r\n",
      "\r\n",
      "\tMPI_Finalize();\t\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "%cat hello.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "def compile(*defs, **defskw):\n",
    "    args = [f\"-D{k}\" for k in defs] + [f\"-D{k}={v}\" for k, v in defskw.items()]\n",
    "    _cmd = 'mpicc -o hello hello.c'.split() + args\n",
    "    # print(' '.join(_cmd))\n",
    "    cmd = subprocess.run(_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "  \n",
    "    if(cmd.stdout): print('cmd.stdout', cmd.stdout)\n",
    "    if(cmd.stderr): print('cmd.stderr', cmd.stderr)\n",
    "    \n",
    "def run(env=None):\n",
    "    cmd = subprocess.run('mpiexec -np 2 ./hello'.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n",
    "    if(cmd.stderr): print('cmd.stderr', cmd.stderr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Длина сообщения\n",
    "\n",
    "Давайте проанализируем влияние длины сообщения на производительность приложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using message length 1\n",
      "\t11.1 ms ± 218 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Using message length 10\n",
      "\t11.1 ms ± 222 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Using message length 100\n",
      "\t11.3 ms ± 249 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Using message length 1000\n",
      "\t11.1 ms ± 343 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Using message length 10000\n",
      "\t13.3 ms ± 481 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Using message length 100000\n",
      "\t27.6 ms ± 1.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Using message length 1000000\n",
      "\t166 ms ± 6.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Using message length 10000000\n",
      "\t5.46 ms ± 260 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(8):\n",
    "    compile(MSG_LEN=10**i)\n",
    "    print(f\"Using message length {10**i}\", end=\"\\n\\t\")\n",
    "    %timeit run()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## МПЦ отправка метод\n",
    "\n",
    "Проанализируем влияние метода send на производительность приложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPI_Rsend as send function\n",
      "\t5.37 ms ± 232 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Using MPI_Ssend as send function\n",
      "\t5.72 ms ± 378 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Using MPI_Send as send function\n",
      "\t5.66 ms ± 204 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for snd in 'MPI_Rsend MPI_Ssend MPI_Send'.split():\n",
    "\n",
    "    compile(MSG_LEN=10**8, SEND_FN=snd)\n",
    "    \n",
    "    print(f\"Using {snd} as send function\", end=\"\\n\\t\")\n",
    "    %timeit run()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SYNC vs ASYNC vs SENDRECV \n",
    "Позволяет анализировать влияние синхронизации на производительность и отправлять сообщения recv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SYNC\n",
      "\t5.94 ms ± 154 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Using ASYNC\n",
      "\t5.77 ms ± 90.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Using SEND_RECV\n",
      "\t5.91 ms ± 165 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for snd in 'SYNC ASYNC SEND_RECV'.split():\n",
    "    \n",
    "    compile(snd, MSG_LEN=10**8)\n",
    "    \n",
    "    print(f\"Using {snd}\", end=\"\\n\\t\")\n",
    "    %timeit run()\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ВЫВОД\n",
    "\n",
    "Лучшая производительность, вероятно, если вы можете написать свою программу, то вы могли использовать только MPI_Ssend для больших данных, а для маленьких MPI_Send работает лучше, потому что для больших данных MPI_Ssend может полностью избежать буферизации данных. В то время как MPI_Send позволяет реализации MPI максимальную гибкость в выборе способа доставки данных. Можно использовать MPI_Bsend только тогда, когда слишком неудобно использовать MPI_Isend, поскольку MPI_Bsend немедленно возвращает буфер. Остальные подпрограммы MPI_Send, MPI_Issend и т. д., редко используются, но могут иметь значение при написании зависящего от системы кода передачи сообщений полностью в MPI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
